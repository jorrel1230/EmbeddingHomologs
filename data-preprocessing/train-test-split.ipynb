{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>fa</th>\n",
       "      <th>sf</th>\n",
       "      <th>fold</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Q03131</td>\n",
       "      <td>4000119</td>\n",
       "      <td>3000038</td>\n",
       "      <td>2000148</td>\n",
       "      <td>MSGPRSRTTSRRTPVRIGAVVVASSTSELLDGLAAVADGRPHASVV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>P09147</td>\n",
       "      <td>4000088</td>\n",
       "      <td>3000038</td>\n",
       "      <td>2000148</td>\n",
       "      <td>MRVLVTGGSGYIGSHTCVQLLQNGHDVIILDNLCNSKRSVLPVIER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>P61889</td>\n",
       "      <td>4000045</td>\n",
       "      <td>3000039</td>\n",
       "      <td>2000005</td>\n",
       "      <td>MKVAVLGAAGGIGQALALLLKTQLPSGSELSLYDIAPVTPGVAVDL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>P00334</td>\n",
       "      <td>4000029</td>\n",
       "      <td>3000038</td>\n",
       "      <td>2000148</td>\n",
       "      <td>MSFTLTNKNVIFVAGLGGIGLDTSKELLKRDLKNLVILDRIENPAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>O33830</td>\n",
       "      <td>4000089</td>\n",
       "      <td>3000039</td>\n",
       "      <td>2000005</td>\n",
       "      <td>MPSVKIGIIGAGSAVFSLRLVSDLCKTPGLSGSTVTLMDIDEERLD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     uid       fa       sf     fold  \\\n",
       "0      0  Q03131  4000119  3000038  2000148   \n",
       "1      1  P09147  4000088  3000038  2000148   \n",
       "2      2  P61889  4000045  3000039  2000005   \n",
       "3      3  P00334  4000029  3000038  2000148   \n",
       "4      4  O33830  4000089  3000039  2000005   \n",
       "\n",
       "                                                 seq  \n",
       "0  MSGPRSRTTSRRTPVRIGAVVVASSTSELLDGLAAVADGRPHASVV...  \n",
       "1  MRVLVTGGSGYIGSHTCVQLLQNGHDVIILDNLCNSKRSVLPVIER...  \n",
       "2  MKVAVLGAAGGIGQALALLLKTQLPSGSELSLYDIAPVTPGVAVDL...  \n",
       "3  MSFTLTNKNVIFVAGLGGIGLDTSKELLKRDLKNLVILDRIENPAA...  \n",
       "4  MPSVKIGIIGAGSAVFSLRLVSDLCKTPGLSGSTVTLMDIDEERLD...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scop_data_fold_path = '/scratch/gpfs/jr8867/strat-main/db/scop_data_fold.csv'\n",
    "scop_data_fold = pd.read_csv(scop_data_fold_path)\n",
    "scop_data_fold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35977, 1280)\n",
      "(35977,)\n"
     ]
    }
   ],
   "source": [
    "embeddings_path = '/scratch/gpfs/jr8867/strat-main/db/full/embeddings.npy'\n",
    "indicies_path = '/scratch/gpfs/jr8867/strat-main/db/full/indicies.npy'\n",
    "embeddings = np.load(embeddings_path)\n",
    "original_indices_npy = np.load(indicies_path)\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(original_indices_npy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique families: 5887\n",
      "Found 927 folds with only one family. These will be excluded from the split.\n",
      "Families available for splitting: 4960\n",
      "\n",
      "Families in train set: 3968\n",
      "Families in test set: 992\n",
      "\n",
      "Initial Train DataFrame shape (before alignment): (24195, 6)\n",
      "Initial Test DataFrame shape (before alignment): (7251, 6)\n",
      "\n",
      "Final Aligned Train DataFrame shape: (24195, 6)\n",
      "Final Aligned Test DataFrame shape: (7251, 6)\n",
      "\n",
      "Final Train embeddings shape: (24195, 1280)\n",
      "Final Test embeddings shape: (7251, 1280)\n",
      "Final Train original indices shape: (24195,)\n",
      "Final Test original indices shape: (7251,)\n",
      "\n",
      "Fold Distribution Comparison for Split Folds (%):\n",
      "         Original % (Split Folds)   Train %    Test %\n",
      "fold                                                 \n",
      "2000000                  1.621828  1.066336  3.475383\n",
      "2000001                  0.031801  0.041331  0.000000\n",
      "2000002                  1.017618  0.103327  4.068404\n",
      "2000003                  0.206704  0.247985  0.068956\n",
      "2000005                  1.418304  1.210994  2.110054\n",
      "\n",
      "Checking train family 4004923:\n",
      "  In final train_df: True\n",
      "  In final test_df: False\n",
      "\n",
      "Checking test family 4003589:\n",
      "  In final train_df: False\n",
      "  In final test_df: True\n"
     ]
    }
   ],
   "source": [
    "# --- Stratified Train/Test Split ---\n",
    "\n",
    "# 1. Create a mapping from family to fold\n",
    "family_to_fold = scop_data_fold[['fa', 'fold']].drop_duplicates().set_index('fa')['fold']\n",
    "print(f\"\\nTotal unique families: {len(family_to_fold)}\")\n",
    "\n",
    "# 2. Identify folds with only one family (these cannot be stratified)\n",
    "fold_family_counts = family_to_fold.groupby(family_to_fold).count()\n",
    "singleton_folds = fold_family_counts[fold_family_counts == 1].index\n",
    "print(f\"Found {len(singleton_folds)} folds with only one family. These will be excluded from the split.\")\n",
    "# print(\"Singleton folds:\", singleton_folds.tolist()) # Optional: print the specific folds\n",
    "\n",
    "# 3. Filter out families belonging to singleton folds\n",
    "families_to_split = family_to_fold[~family_to_fold.isin(singleton_folds)]\n",
    "print(f\"Families available for splitting: {len(families_to_split)}\")\n",
    "\n",
    "# 4. Get unique families (eligible for splitting) and their folds for stratification\n",
    "unique_families_to_split = families_to_split.index.unique()\n",
    "family_strata = families_to_split[unique_families_to_split] # Folds corresponding to the families being split\n",
    "\n",
    "# 5. Perform stratified split on the eligible families\n",
    "train_families, test_families = train_test_split(\n",
    "    unique_families_to_split,\n",
    "    test_size=0.2,\n",
    "    stratify=family_strata,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nFamilies in train set: {len(train_families)}\")\n",
    "print(f\"Families in test set: {len(test_families)}\")\n",
    "\n",
    "# 6. Filter the *original* DataFrame based on the family split\n",
    "# Include data from singleton folds in the training set by default, or handle them separately if needed\n",
    "# Here, we'll just use the families selected for train/test\n",
    "train_df = scop_data_fold[scop_data_fold['fa'].isin(train_families)].reset_index(drop=True)\n",
    "test_df = scop_data_fold[scop_data_fold['fa'].isin(test_families)].reset_index(drop=True)\n",
    "\n",
    "# Optional: Add families from singleton folds back to the training set\n",
    "# singleton_families = family_to_fold[family_to_fold.isin(singleton_folds)].index\n",
    "# singleton_df = scop_data_fold[scop_data_fold['fa'].isin(singleton_families)]\n",
    "# train_df = pd.concat([train_df, singleton_df]).reset_index(drop=True)\n",
    "# print(f\"Added {len(singleton_families)} families from singleton folds to the training set.\")\n",
    "\n",
    "\n",
    "print(f\"\\nInitial Train DataFrame shape (before alignment): {train_df.shape}\")\n",
    "print(f\"Initial Test DataFrame shape (before alignment): {test_df.shape}\")\n",
    "\n",
    "# Check if the split was successful (no overlapping families among split sets)\n",
    "assert len(set(train_df['fa']).intersection(set(test_df['fa']))) == 0, \"Overlap detected in families between train and test sets!\"\n",
    "\n",
    "# --- Align and Split Embeddings ---\n",
    "\n",
    "# 7. Create a map from original index value to its position in the embeddings array\n",
    "index_map = {original_idx: position for position, original_idx in enumerate(original_indices_npy)}\n",
    "\n",
    "# 8. Get the original index values present in the train/test DataFrames\n",
    "train_indices_df = train_df['index'].values\n",
    "test_indices_df = test_df['index'].values\n",
    "\n",
    "# 9. Find the positions in the embeddings array corresponding to train/test data\n",
    "train_positions = [index_map[idx] for idx in train_indices_df if idx in index_map]\n",
    "test_positions = [index_map[idx] for idx in test_indices_df if idx in index_map]\n",
    "\n",
    "# 10. Filter DataFrames to only include samples whose embeddings were found\n",
    "original_train_size = len(train_df)\n",
    "original_test_size = len(test_df)\n",
    "\n",
    "# Important: Filter based on the indices *actually found* in the map\n",
    "train_indices_found = {original_indices_npy[pos] for pos in train_positions}\n",
    "test_indices_found = {original_indices_npy[pos] for pos in test_positions}\n",
    "\n",
    "train_df = train_df[train_df['index'].isin(train_indices_found)].reset_index(drop=True)\n",
    "test_df = test_df[test_df['index'].isin(test_indices_found)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "if len(train_df) != original_train_size:\n",
    "    print(f\"\\nWarning: {original_train_size - len(train_df)} train samples dropped due to index mismatch or belonging to excluded singleton families.\")\n",
    "if len(test_df) != original_test_size:\n",
    "     print(f\"Warning: {original_test_size - len(test_df)} test samples dropped due to index mismatch or belonging to excluded singleton families.\")\n",
    "\n",
    "print(f\"\\nFinal Aligned Train DataFrame shape: {train_df.shape}\")\n",
    "print(f\"Final Aligned Test DataFrame shape: {test_df.shape}\")\n",
    "\n",
    "\n",
    "# 11. Slice the embeddings and original index arrays using the found positions\n",
    "train_embeddings = embeddings[train_positions]\n",
    "test_embeddings = embeddings[test_positions]\n",
    "\n",
    "# Optional: Keep the corresponding original indices as well\n",
    "train_original_indices = original_indices_npy[train_positions]\n",
    "test_original_indices = original_indices_npy[test_positions]\n",
    "\n",
    "# --- Final Checks ---\n",
    "assert train_embeddings.shape[0] == len(train_df), \"Train embeddings rows do not match Train DataFrame rows\"\n",
    "assert test_embeddings.shape[0] == len(test_df), \"Test embeddings rows do not match Test DataFrame rows\"\n",
    "assert train_original_indices.shape[0] == len(train_df), \"Train original indices count does not match Train DataFrame rows\"\n",
    "assert test_original_indices.shape[0] == len(test_df), \"Test original indices count does not match Test DataFrame rows\"\n",
    "\n",
    "\n",
    "print(f\"\\nFinal Train embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"Final Test embeddings shape: {test_embeddings.shape}\")\n",
    "print(f\"Final Train original indices shape: {train_original_indices.shape}\")\n",
    "print(f\"Final Test original indices shape: {test_original_indices.shape}\")\n",
    "\n",
    "\n",
    "# --- Verification (Optional) ---\n",
    "\n",
    "# Verify stratification (only on folds that were actually split)\n",
    "split_folds = family_strata.unique()\n",
    "original_fold_dist = scop_data_fold[scop_data_fold['fold'].isin(split_folds)]['fold'].value_counts(normalize=True)\n",
    "train_fold_dist = train_df[train_df['fold'].isin(split_folds)]['fold'].value_counts(normalize=True)\n",
    "test_fold_dist = test_df[test_df['fold'].isin(split_folds)]['fold'].value_counts(normalize=True)\n",
    "\n",
    "fold_dist_comparison = pd.DataFrame({\n",
    "    'Original % (Split Folds)': original_fold_dist * 100,\n",
    "    'Train %': train_fold_dist * 100,\n",
    "    'Test %': test_fold_dist * 100\n",
    "}).fillna(0)\n",
    "\n",
    "print(\"\\nFold Distribution Comparison for Split Folds (%):\")\n",
    "print(fold_dist_comparison.head())\n",
    "\n",
    "# Verify family separation: Check a few families are entirely in one set\n",
    "if len(train_families) > 0:\n",
    "    example_train_family = train_families[0]\n",
    "    print(f\"\\nChecking train family {example_train_family}:\")\n",
    "    print(f\"  In final train_df: {train_df[train_df['fa'] == example_train_family].shape[0] > 0}\")\n",
    "    print(f\"  In final test_df: {test_df[test_df['fa'] == example_train_family].shape[0] > 0}\")\n",
    "\n",
    "if len(test_families) > 0:\n",
    "    example_test_family = test_families[0]\n",
    "    print(f\"\\nChecking test family {example_test_family}:\")\n",
    "    print(f\"  In final train_df: {train_df[train_df['fa'] == example_test_family].shape[0] > 0}\")\n",
    "    print(f\"  In final test_df: {test_df[test_df['fa'] == example_test_family].shape[0] > 0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved train and test data to /scratch/gpfs/jr8867/main/db/family-split-train-test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_split_path = '/scratch/gpfs/jr8867/main/db/family-split-train-test' # directory to save the split data\n",
    "# Create the save directory if it doesn't exist\n",
    "os.makedirs(save_split_path, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(save_split_path, 'train')\n",
    "test_dir = os.path.join(save_split_path, 'test')\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Save train and test DataFrames to CSV\n",
    "train_df.to_csv(os.path.join(train_dir, 'train_metadata.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(test_dir, 'test_metadata.csv'), index=False)\n",
    "\n",
    "# Save embeddings to NumPy .npy files\n",
    "np.save(os.path.join(train_dir, 'train_embeddings.npy'), train_embeddings)\n",
    "np.save(os.path.join(test_dir, 'test_embeddings.npy'), test_embeddings)\n",
    "\n",
    "# Save original indices to NumPy .npy files\n",
    "np.save(os.path.join(train_dir, 'train_original_indices.npy'), train_original_indices)\n",
    "np.save(os.path.join(test_dir, 'test_original_indices.npy'), test_original_indices)\n",
    "\n",
    "print(f\"\\nSaved train and test data to {save_split_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set size: 7251\n",
      "Original train set size: 24195\n",
      "\n",
      "Created training subset with size: 7251\n",
      "Train subset DataFrame shape: (7251, 6)\n",
      "Train subset embeddings shape: (7251, 1280)\n",
      "Train subset original indices shape: (7251,)\n",
      "\n",
      "Training subset created and verified successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Create a Training Subset Matching Test Set Size ---\n",
    "\n",
    "# Get the size of the test set\n",
    "n_test_samples = len(test_df)\n",
    "n_train_samples = len(train_df)\n",
    "\n",
    "print(f\"\\nTest set size: {n_test_samples}\")\n",
    "print(f\"Original train set size: {n_train_samples}\")\n",
    "\n",
    "if n_test_samples > n_train_samples:\n",
    "    print(\"Warning: Test set is larger than the training set. Cannot create a subset of this size.\")\n",
    "    # Handle this case as needed, maybe skip subset creation\n",
    "    train_subset_df = None\n",
    "    train_subset_embeddings = None\n",
    "    train_subset_original_indices = None\n",
    "else:\n",
    "    # Generate random indices for the subset (without replacement)\n",
    "    subset_indices = np.random.choice(n_train_samples, size=n_test_samples, replace=False)\n",
    "    subset_indices.sort() # Optional: sort indices for potential minor efficiency gains\n",
    "\n",
    "    # Create the subset DataFrame and embeddings\n",
    "    train_subset_df = train_df.iloc[subset_indices].reset_index(drop=True)\n",
    "    train_subset_embeddings = train_embeddings[subset_indices]\n",
    "    train_subset_original_indices = train_original_indices[subset_indices]\n",
    "\n",
    "    print(f\"\\nCreated training subset with size: {len(train_subset_df)}\")\n",
    "    print(f\"Train subset DataFrame shape: {train_subset_df.shape}\")\n",
    "    print(f\"Train subset embeddings shape: {train_subset_embeddings.shape}\")\n",
    "    print(f\"Train subset original indices shape: {train_subset_original_indices.shape}\")\n",
    "\n",
    "    # --- Verification ---\n",
    "    assert len(train_subset_df) == n_test_samples, \"Subset size does not match test set size\"\n",
    "    assert train_subset_embeddings.shape[0] == n_test_samples, \"Subset embeddings rows do not match test set size\"\n",
    "    assert train_subset_original_indices.shape[0] == n_test_samples, \"Subset original indices count does not match test set size\"\n",
    "\n",
    "    # Assert there is no overlap between train subset and test set\n",
    "    assert len(set(train_subset_df['fa']).intersection(set(test_df['fa']))) == 0, \"Overlap detected in families between train subset and test set!\"\n",
    "\n",
    "    print(\"\\nTraining subset created and verified successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved training subset data to /scratch/gpfs/jr8867/main/db/family-split-train-test/train_subset\n"
     ]
    }
   ],
   "source": [
    "# --- Save Training Subset (Optional) ---\n",
    "\n",
    "if train_subset_df is not None:\n",
    "    subset_save_dir = os.path.join(save_split_path, 'train_subset')\n",
    "    os.makedirs(subset_save_dir, exist_ok=True)\n",
    "\n",
    "    # Save subset DataFrame to CSV\n",
    "    train_subset_df.to_csv(os.path.join(subset_save_dir, 'train_subset_metadata.csv'), index=False)\n",
    "\n",
    "    # Save subset embeddings to NumPy .npy file\n",
    "    np.save(os.path.join(subset_save_dir, 'train_subset_embeddings.npy'), train_subset_embeddings)\n",
    "\n",
    "    # Save subset original indices to NumPy .npy file\n",
    "    np.save(os.path.join(subset_save_dir, 'train_subset_original_indices.npy'), train_subset_original_indices)\n",
    "\n",
    "    print(f\"\\nSaved training subset data to {subset_save_dir}\")\n",
    "else:\n",
    "    print(\"\\nSkipping saving of training subset as it was not created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information\n",
    "\n",
    "Embeddings npy objects are lined up row by row with the test/train df."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
